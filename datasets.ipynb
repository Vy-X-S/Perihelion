{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def read_jsonl(file_path, number_of_lines = 100):\n",
    "    \"\"\"\n",
    "    Reads a specified number of lines from a JSON Lines file and splits the data into train and test sets.\n",
    "\n",
    "    :param file_path: Path to the JSON Lines file.\n",
    "    :param number_of_lines: Number of lines to read from the file.\n",
    "    :return: DataFrame \n",
    "    \"\"\"\n",
    "    \n",
    "    # Read specified number of lines from file\n",
    "    with open(file_path) as file:\n",
    "        lines = [json.loads(next(file)) for _ in range(number_of_lines)]\n",
    "        \n",
    "    # Convert list of JSON objects to Pandas DF\n",
    "    data = pd.DataFrame(lines)\n",
    "    \n",
    "    return data\n",
    "\n",
    "data = read_jsonl(\"dataset/grocery_fixed.jsonl\", 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Preprocessing \n",
    "\n",
    "def pre_process(text):\n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # remove tags\n",
    "    text = re.sub(\"&lt;/?.*&gt;\",\" &lt;&gt; \", text)\n",
    "    \n",
    "    # remove special characters \n",
    "    text = re.sub(\"(\\\\d|\\\\W)+\",\" \", text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "data['text'] = data['title'] + \" \" + data['text']\n",
    "data['text'] = data['text'].apply(pre_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key = lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \"\"\" Get feature names and tf-idf scores of top n items\"\"\"\n",
    "    sorted_items = sorted_items[:topn]\n",
    "    \n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    \n",
    "    for idx, score in sorted_items:\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    "    \n",
    "    results = {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]] = score_vals[idx]\n",
    "    \n",
    "    return results\n",
    "\n",
    "def get_stop_words(stop_file_path):\n",
    "    \"\"\"Load stop words\"\"\"\n",
    "    with open(stop_file_path, 'r', encoding='utf-8') as f:\n",
    "        stopwords = f.readlines()\n",
    "        stop_set = [m.strip() for m in stopwords]\n",
    "        return stop_set  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures\n",
    "from nltk.collocations import TrigramCollocationFinder, TrigramAssocMeasures\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def extract_keywords_for_product(data, asin, topn=10):\n",
    "    \"\"\"\n",
    "    Extract top n keywords for a specific product based on its reviews.\n",
    "    \n",
    "    :param data: The dataset containing reviews.\n",
    "    :param asin: The product identifier (e.g., ASIN) to extract keywords for.\n",
    "    :param topn: The number of top keywords to extract.\n",
    "    :return: A dictionary of top n keywords and their TF-IDF scores for the specified product.\n",
    "    \"\"\"\n",
    "    # Load set of stop words\n",
    "    stopwords = get_stop_words(\"dataset/stopwords.txt\")\n",
    "\n",
    "    # Initialize TF-IDF Vectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=0.85, stop_words=stopwords, max_features=10000)\n",
    "\n",
    "    # Filter reviews for the specified product\n",
    "    product_data = data.loc[data['parent_asin'] == asin]\n",
    "    docs = product_data['text'].tolist()\n",
    "\n",
    "    # Compute TF-IDF matrix\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(docs)\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Sort items by TF-IDF score\n",
    "    sorted_items = sort_coo(tfidf_matrix.tocoo())\n",
    "\n",
    "    # Extract top n keywords\n",
    "    keywords = extract_topn_from_vector(feature_names, sorted_items, topn=topn)\n",
    "\n",
    "    return keywords\n",
    "def get_top_ngrams_for_product(reviews, topn=10, min_freq=3):\n",
    "    \"\"\"\n",
    "    Get the top bigrams and trigrams for a specific product's reviews.\n",
    "    \n",
    "    :param reviews: List of reviews for a specific product.\n",
    "    :param topn: Number of top n-grams to return.\n",
    "    :param min_freq: Minimum frequency for n-grams to be considered.\n",
    "    :return: A tuple containing lists of the top bigrams and trigrams.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Preprocess and tokenize reviews\n",
    "    tokenized_reviews = [word_tokenize(review) for review in reviews]\n",
    "    \n",
    "    # Flatten list of tokenized reviews into one list of words\n",
    "    all_words = [word for tokens in tokenized_reviews for word in tokens]\n",
    "    \n",
    "    # Instantiate collocation finders\n",
    "    bigram_finder = BigramCollocationFinder.from_words(all_words)\n",
    "    trigram_finder = TrigramCollocationFinder.from_words(all_words)\n",
    "    \n",
    "    # Apply frequency filter \n",
    "    bigram_finder.apply_freq_filter(min_freq)\n",
    "    trigram_finder.apply_freq_filter(min_freq)\n",
    "    \n",
    "    # Measures for calculating PMI\n",
    "    bigram_measures = BigramAssocMeasures()\n",
    "    trigram_measures = TrigramAssocMeasures()\n",
    "    \n",
    "    # Extract top n-grams with highest PMI \n",
    "    top_bigrams = bigram_finder.nbest(bigram_measures.pmi, topn)\n",
    "    top_trigrams = trigram_finder.nbest(trigram_measures.pmi, topn)\n",
    "    \n",
    "    return top_bigrams, top_trigrams\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "parent_asin\n",
       "B07LFJF6TR    3973\n",
       "B00ESE0DC4    3443\n",
       "B07MDTNZ66    2535\n",
       "B01NAYX4S3    2341\n",
       "B0BZZWHKHQ    2330\n",
       "              ... \n",
       "B01HOER10Q       1\n",
       "B00HRG4ORU       1\n",
       "B00I2ZWD9Q       1\n",
       "B08HMV5WSW       1\n",
       "B07C3QCFB3       1\n",
       "Name: count, Length: 196617, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['parent_asin'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product keywords: {'yuk': 1.0, 'thank': 1.0, 'nice': 1.0, 'great': 1.0, 'good': 1.0, 'flavor': 1.0, 'excellent': 1.0, 'delicious': 1.0, 'awesome': 0.992, 'love': 0.975, 'weak': 0.963}\n",
      "\n",
      "Top bigrams: [('tierra', 'intenso'), ('koffee', 'kult'), ('tim', 'hortons'), ('gon', 'na'), ('west', 'coast'), ('costa', 'rican'), ('timely', 'manner'), ('asin', 'b'), ('san', 'antonio'), ('puerto', 'rico'), ('san', 'francisco'), ('caf', 'eacute'), ('ash', 'tray'), ('kicking', 'horse'), ('gas', 'station'), ('bullet', 'proof'), ('jamaican', 'blue'), ('blue', 'mountain'), ('central', 'america'), ('trader', 'joes')]\n",
      "\n",
      "Top trigrams: [('eight', 'o', 'clock'), ('jamaican', 'blue', 'mountain'), ('guatemalan', 'san', 'antonio'), ('peaberry', 'guatemalan', 'san'), ('honduran', 'peaberry', 'guatemalan'), ('burke', 'brands', 'llc'), ('caf', 'eacute', 'don'), ('law', 'said', 'she'), ('sam', 's', 'club'), ('of', 'koffee', 'kult'), ('blown', 'away', 'by'), ('an', 'air', 'tight'), ('worth', 'every', 'penny'), ('anyone', 'who', 'likes'), ('in', 'central', 'america'), ('pour', 'over', 'method'), ('water', 'processed', 'decaf'), ('strands', 'of', 'hair'), ('koffee', 'kult', 'and'), ('a', 'timely', 'manner')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "asin = \"B07LFJF6TR\"\n",
    "topn = 20\n",
    "\n",
    "product_keywords = extract_keywords_for_product(data, asin, topn=topn)\n",
    "\n",
    "product_reviews = data.loc[data['parent_asin'] == asin]['text'].tolist()\n",
    "top_bigrams, top_trigrams = get_top_ngrams_for_product(product_reviews, topn=topn, min_freq=3)\n",
    "\n",
    "print(f\"Product keywords: {product_keywords}\\n\")\n",
    "print(f\"Top bigrams: {top_bigrams}\\n\")\n",
    "print(f\"Top trigrams: {top_trigrams}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vy/workspace/github.com/Vy-X-S/Perihelion/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-04-07 10:30:41.753738: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-07 10:30:42.361967: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-07 10:30:43.736992: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Zero shot classification model\n",
    "\n",
    "from transformers import pipeline \n",
    "model_name = \"facebook/bart-large-mnli\"\n",
    "classifier = pipeline('zero-shot-classification', model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:06<00:00,  1.49it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Filter reviews for the specified product\n",
    "product_data = data.loc[data['parent_asin'] == asin][:100].copy()\n",
    "\n",
    "# Init two new columns for the classification and score\n",
    "product_data['label'] = np.nan\n",
    "product_data['score'] = np.nan\n",
    "\n",
    "# Define candidate labels and text to classify\n",
    "candidate_labels = [\"positive\", \"negative\"]\n",
    "\n",
    "# Classify each review and store the results\n",
    "for index, row in tqdm(product_data.iterrows(), total=product_data.shape[0]):\n",
    "    # Classify the current review text\n",
    "    output = classifier(row['text'], candidate_labels)\n",
    "\n",
    "    # Update the predicted label and score in the DataFrame\n",
    "    product_data.at[index, 'predicted_label'] = output['labels'][0]  \n",
    "    product_data.at[index, 'predicted_score'] = output['scores'][0]  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predicted_label\n",
       "positive    86\n",
       "negative    14\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_data['predicted_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    100.000000\n",
       "mean       0.974206\n",
       "std        0.062934\n",
       "min        0.633873\n",
       "25%        0.985333\n",
       "50%        0.995517\n",
       "75%        0.996761\n",
       "max        0.998831\n",
       "Name: predicted_score, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_data['predicted_score'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 4035, but your input_length is only 852. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=426)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': '\"This coffee is really good try it br long version the chocolate notes really shine and this is a really bold low acidity brew\" \"I can\\'t believe how good this coffee is plus water process decaf and certified organic win win in my book  not much flavor sorry but it tastes like water  very smokey flavor like it s roasted too hot or too long  great deal great tasting coffee  delicious coffee good packaging smells fresh\"'}]\n"
     ]
    }
   ],
   "source": [
    "# BART summarization on aggregated reviews\n",
    "\n",
    "AGGREGATED_REVIEW_TEXT = \" \".join(product_data['text'].tolist())\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "summary = summarizer(AGGREGATED_REVIEW_TEXT, max_length=len(AGGREGATED_REVIEW_TEXT), min_length=30, do_sample=False)\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': '\"This coffee is really good try it br long version the chocolate notes really shine and this is a really bold low acidity brew\" \"I can\\'t believe how good this coffee is plus water process decaf and certified organic win win in my book  not much flavor sorry but it tastes like water  very smokey flavor like it s roasted too hot or too long  great deal great tasting coffee  delicious coffee good packaging smells fresh\"'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
